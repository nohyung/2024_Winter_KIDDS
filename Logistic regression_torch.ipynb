{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression (pytorch)\n",
    "\n",
    "KIDDS 2024\n",
    "\n",
    "Dates: 2024-2-1\n",
    "\n",
    "Author: Yung-Kyun Noh\n",
    "\n",
    "Department of Computer Science, Hanyang University & School of Computational Sciences, KIAS\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "This notebook aims at practicing \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributions as distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selection of GPU can be performed using one of the following settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU selection\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_gpu = 1\n",
    "dev = 'cuda:' + str(run_gpu)\n",
    "# dev='cpu'\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Draw two-class data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_state(wval, bval, data1, data2, title_str='Data'):\n",
    "    # function for scattering data and drawing classification boundary\n",
    "    # wx - b > 0 or  wx - b < 0\n",
    "    \n",
    "    # create a figure and axis\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Scatter data points in 2-dimensional space\n",
    "    ax.scatter(data1[:,0], data1[:,1], label='class 1', c='red', alpha=.3)\n",
    "    ax.scatter(data2[:,0], data2[:,1], label='class 2', marker='^', c='blue', alpha=.3)\n",
    "    # set a title and labels\n",
    "    ax.set_title(title_str)\n",
    "    ax.legend()\n",
    "    \n",
    "    [x1min,x1max,x2min,x2max] = ax.axis()\n",
    "    x1vals = np.arange(x1min,x1max,0.1)\n",
    "    ax.plot(x1vals, (-wval[0]*x1vals + bval)/wval[1], 'k')\n",
    "    ax.axis([x1min,x1max,x2min,x2max])\n",
    "    ax.grid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_function(t):\n",
    "    # example: logistic_function(np.array([0,1,2]))\n",
    "    ret_val = 1/(1 + torch.exp(-t))\n",
    "    return ret_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(labels, fs):\n",
    "    loss_val = torch.sum(-labels*torch.log(fs) - (1 - labels)*torch.log(1 - fs))\n",
    "    return loss_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generation\n",
    "\n",
    "Consider two classes of two-dimensional data, each generated from a Gaussian density function. Each class follows a Gaussian density with the same covariance matrix but different means. Since the two Gaussians share the same covariance matrix, the optimal boundary for discriminating between the two classes is linear.\n",
    "\n",
    "For training, 50 data points are generated for each class, and for testing, 1000 data points are generated. The higher number of synthetic testing data allows for a more confident evaluation of the learned results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate two Gaussians (class 1 & class 2)\n",
    "dim = 2\n",
    "datanum1 = 50\n",
    "datanum2 = 50\n",
    "mean1 = np.array([0., 0.])\n",
    "mean2 = np.array([1., -.5])\n",
    "cov1 = np.array([[.1,.02],[.02,.1]])\n",
    "cov2 = np.array([[.1,.02],[.02,.1]])\n",
    "# float32\n",
    "L = torch.linalg.cholesky(torch.from_numpy(cov1).to(dev)).to(torch.float32)\n",
    "data1 = torch.matmul(torch.randn(datanum1, dim, device=dev, dtype=torch.float32), L.T) \\\n",
    "        + torch.from_numpy(mean1).to(torch.float32).to(dev)\n",
    "L = torch.linalg.cholesky(torch.from_numpy(cov2).to(dev)).to(torch.float32)\n",
    "data2 = torch.matmul(torch.randn(datanum2, dim, device=dev, dtype=torch.float32), L.T) \\\n",
    "        + torch.from_numpy(mean2).to(torch.float32).to(dev)\n",
    "\n",
    "# data1 = np.random.multivariate_normal(mean1, cov1, datanum1)\n",
    "# data2 = np.random.multivariate_normal(mean2, cov2, datanum2)\n",
    "\n",
    "tstdatanum1 = 1000\n",
    "tstdatanum2 = 1000\n",
    "L = torch.linalg.cholesky(torch.from_numpy(cov1).to(dev)).to(torch.float32)\n",
    "tstdata1 = torch.matmul(torch.randn(tstdatanum1, dim, device=dev, dtype=torch.float32), L.T) \\\n",
    "        + torch.from_numpy(mean1).to(torch.float32).to(dev)\n",
    "L = torch.linalg.cholesky(torch.from_numpy(cov2).to(dev)).to(torch.float32)\n",
    "tstdata2 = torch.matmul(torch.randn(tstdatanum2, dim, device=dev, dtype=torch.float32), L.T) \\\n",
    "        + torch.from_numpy(mean2).to(torch.float32).to(dev)\n",
    "\n",
    "# tstdata1 = np.random.multivariate_normal(mean1, cov1, tstdatanum1)\n",
    "# tstdata2 = np.random.multivariate_normal(mean2, cov2, tstdatanum2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal Bayes classifier\n",
    "\n",
    "When the data-generating functions are Gaussians with equivalent covariances, we can obtain the Bayes optimal linear classifier from the parameters. Let $p_1(\\mathbf{x}) = \\mathcal{N}(\\mu_1, \\Sigma)$ and $p_2(\\mathbf{x}) = \\mathcal{N}(\\mu_2, \\Sigma)$ with shared $\\Sigma$, the classification rule  is as follows:\n",
    "\\begin{eqnarray}\n",
    "y = 1 \\quad \\mathrm{if} \\ \\mathbf{w}^\\top\\mathbf{x} - b \\geq 0 \\nonumber \\\\\n",
    "y = 0 \\quad \\mathrm{if} \\ \\mathbf{w}^\\top\\mathbf{x} - b < 0\n",
    "\\end{eqnarray}\n",
    "with\n",
    "\\begin{eqnarray}\n",
    "\\mathbf{w} = \\Sigma^{-1}(\\mu_1 - \\mu_2), \\quad b = \\frac{1}{2}\\left(\\mu_1 - \\mu_2 \\right).\n",
    "\\end{eqnarray}\n",
    "This classifier optimally predicts the two classes based on the given Gaussian densities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimal linear classifier\n",
    "optw = np.matmul(mean1 - mean2, np.linalg.inv(cov1))\n",
    "optb = np.matmul(optw, (mean1 + mean2)/2)\n",
    "print(optw, optb)\n",
    "\n",
    "draw_state(optw, optb, data1.cpu(), data2.cpu(), 'Data and optimal boundary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "n:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try several different levels of coding using pyTorch in order to obtain $\\mathbf{w}$ and $b$ out of data. First, the subsequent update uses the gradient vector derived by taking the derivative of the objective function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We consider the following regularized cross entropy objective:\n",
    "\\begin{eqnarray}\n",
    "L = -\\frac{1}{N}\\sum_{i = 1}^N y\\log f(\\mathbf{x}; \\mathbf{w}, b) + (1 - y)\\log (1 - f(\\mathbf{x}; \\mathbf{w}, b)) \n",
    "+ \\lambda(||\\mathbf{w}||^2 + b^2),\n",
    "\\end{eqnarray}\n",
    "with\n",
    "\\begin{eqnarray}\n",
    "f(\\mathbf{x}; \\mathbf{w}, b) = \\frac{1}{1 + \\exp(\\mathbf{w}^\\top\\mathbf{x} - b)},\n",
    "\\end{eqnarray}\n",
    "and a small positive regularization constant $\\lambda > 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "w = torch.randn([1,dim], dtype=torch.float32, device=dev, requires_grad=True)\n",
    "b = torch.randn([1,1], dtype=torch.float32, device=dev, requires_grad=True)\n",
    "\n",
    "# extended w: [w, -b]\n",
    "extw = torch.cat([w, -b], axis=1)\n",
    "# data with '1' is appended: [X, 1]\n",
    "extX = torch.cat((torch.cat([data1, data2], axis=0), \\\n",
    "                       torch.ones([datanum1 + datanum2, 1], device=dev)), axis=1)\n",
    "labels = torch.cat([torch.ones(datanum1, device=dev), torch.zeros(datanum2, device=dev)])  # label of class 1: 1, label of class 2: 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the derivative of the loss function to update\n",
    "\n",
    "The standard update of the parameters is as follows:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\left(\n",
    "\\begin{array}{c}\n",
    "\\mathbf{w}_{t + 1} \\\\\n",
    "b_{t + 1}\n",
    "\\end{array}\n",
    "\\right) = \n",
    "\\left(\n",
    "\\begin{array}{c}\n",
    "\\mathbf{w}_{t} \\\\\n",
    "b_{t}\n",
    "\\end{array}\n",
    "\\right) - \n",
    "\\epsilon\n",
    "\\left(\n",
    "\\begin{array}{c}\n",
    "\\frac{dL}{d\\mathbf{w}} \\\\\n",
    "\\frac{dL}{db} \\\\\n",
    "\\end{array}\n",
    "\\right),\n",
    "\\end{eqnarray}\n",
    "with a small positive constant $\\epsilon > 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we can use a simpler code. We let $\\mathbf{w}' = \\left(\\begin{array}{c}\\mathbf{w} \\\\ -b \\end{array} \\right)$, the $f(\\mathbf{x}; \\mathbf{w}, b)$ function can be rewritten using $\\mathbf{w}'$ as\n",
    "\\begin{eqnarray}\n",
    "f(\\mathbf{x}; \\mathbf{w}') = f(\\mathbf{x}; \\mathbf{w}, b) = \\frac{1}{1 + \\exp(\\mathbf{w}^\\top\\mathbf{x} - b)}\n",
    "= \\frac{1}{1 + \\exp\\left(\\mathbf{w}'^\\top\\left(\\begin{array}{c}\\mathbf{x} \\\\ 1 \\end{array}\\right)\\right)},\n",
    "\\end{eqnarray}\n",
    "\n",
    "We can derive $\\frac{dL}{d\\mathbf{w}'}$ and make a new update rule.\n",
    "\\begin{eqnarray}\n",
    "\\mathbf{w}'_{t + 1} = \\mathbf{w}_t - \\epsilon \\frac{dL}{d\\mathbf{w}'},\n",
    "\\end{eqnarray}\n",
    "with a small positive $\\epsilon > 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_w_vanilla(extw, extX, labels, num_update=100, step_size=0.001, reg_const=1):\n",
    "    draw_state(extw[0,0:2].cpu().detach().numpy(), -extw[0,2].cpu().detach().numpy(), data1.cpu(), data2.cpu(), 'Before update')\n",
    "\n",
    "    objective_history = []\n",
    "    for i in range(num_update):\n",
    "        ts = torch.matmul(extX, extw.T).T  # w^TX\n",
    "        fs = logistic_function(ts)\n",
    "        extw = extw + step_size*(torch.matmul(labels - fs, extX) - reg_const*extw)\n",
    "        objective_history.append(float(get_loss(labels, fs[0]).cpu().detach().numpy()))\n",
    "\n",
    "    draw_state(extw[0,0:2].cpu().detach().numpy(), -extw[0,2].cpu().detach().numpy(), data1.cpu(), data2.cpu(), 'Updated boundary')\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(objective_history)\n",
    "    ax.set_title(\"objective function\")\n",
    "    \n",
    "    return extw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we implemented our manual update code with the derivatives $\\frac{dL}{d\\mathbf{w}}$ and $\\frac{dL}{db}$ that have been devied by us.\n",
    "\n",
    "The update of the parameters show the update of the boundary. The boundary is determined by $\\mathbf{w}$ and $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extw = update_w_vanilla(extw, extX, labels, num_update=100, step_size=0.001, reg_const=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further update show improved results, and the objective function value keep decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional 100 updates\n",
    "extw = update_w_vanilla(extw, extX, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After enough updates\n",
    "extw = update_w_vanilla(extw, extX, labels, num_update=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Now we use the testing data to estimate the expected error for untrained data. The error is approximated $3\\%$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data with '1' is appended: [X, 1]\n",
    "extTstX = torch.cat([torch.cat([tstdata1, tstdata2], axis=0), \\\n",
    "                       torch.ones([tstdatanum1 + tstdatanum2, 1], device=dev)], axis=1)\n",
    "labels = torch.cat([torch.ones(tstdatanum1, device=dev), torch.zeros(tstdatanum2, device=dev)])\n",
    "\n",
    "ts = torch.matmul(extTstX, extw.T)  # w^TX\n",
    "err_rate = torch.sum(torch.abs((ts.T > 0)*1. - labels))/(tstdatanum1 + tstdatanum2)\n",
    "print(err_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use torch optimization\n",
    "\n",
    "We implemented the gradient descent algorithm using python code. Now we borrow some pre-implemented codes from pyTorch and see how the update in the previous example can be performed using basic optimization functions in pyTorch.\n",
    "\n",
    "The following code performs one step of the update using the predefined information in optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "        cost = model.get_loss(data, labels)\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first make a sample class that keeps the parameters of prediction function $f(\\mathbf{x}; \\mathbf{w}, b)$ in the above example. Forward function implements the calculation of the function $f(\\mathbf{x}; \\mathbf{w}, b)$ with input $\\mathbf{x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For test 1\n",
    "class LogisticRegression_torch_0(torch.nn.Module):    \n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression_torch_0, self).__init__()\n",
    "        self.w = torch.randn([1,dim], dtype=torch.float32, device=dev, requires_grad=True)\n",
    "        self.b = torch.randn([1,1], dtype=torch.float32, device=dev, requires_grad=True)\n",
    "    def forward(self, x):\n",
    "        outputs = torch.sigmoid(torch.matmul(self.w, x.T) - self.b)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "When we initialize the class, the parameters $\\mathbf{w}$ and $b$ are also initialized with random floats. The update parameters should have the option \"requres_grad=True.\"\n",
    "\n",
    "In our test of pyTorch optimization functions, we use the two-norm of $\\mathbf{w}$ ($=||\\mathrm{w}||^2$) as our cost. Let's observe how the pyTorch optimization functions decrease the cost after a single step is taken.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1\n",
    "model = LogisticRegression_torch_0(input_dim=dim, output_dim=1).to(dev)\n",
    "\n",
    "cost = torch.sum(model.w**2)\n",
    "print(\"Cost before taking a single optimization step:\", cost.item())\n",
    "print('w:', model.w.cpu().detach().numpy())\n",
    "\n",
    "optimizer = torch.optim.SGD([model.w, model.b], lr=0.01)\n",
    "optimizer.zero_grad()\n",
    "cost.backward()\n",
    "optimizer.step()\n",
    "\n",
    "cost = torch.sum(model.w**2)\n",
    "print(\"Cost after taking a single optimization step:\", cost.item())\n",
    "print('w:', model.w.cpu().detach().numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now construct a class with an implementation of the objective function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    " get_loss(self, data, labels)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss is the cross entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For test 1\n",
    "class LogisticRegression_torch1(torch.nn.Module):    \n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression_torch1, self).__init__()\n",
    "        self.w = torch.randn([1,dim], dtype=torch.float32, device=dev, requires_grad=True)\n",
    "        self.b = torch.randn([1,1], dtype=torch.float32, device=dev, requires_grad=True)\n",
    "    def get_loss(self, data, labels):\n",
    "        fs = self.forward(data)\n",
    "        loss_val = torch.sum(-labels*torch.log(fs) - (1 - labels)*torch.log(1 - fs))\n",
    "        return loss_val\n",
    "    def forward(self, x):\n",
    "        outputs = torch.sigmoid(torch.matmul(self.w, x.T) - self.b)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A series of updates is performed in the for loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_w_torch1(model, data, labels, num_update=100, step_size=0.001):\n",
    "    draw_state(model.w[0].cpu().detach().numpy(), model.b[0].cpu().detach().numpy(), data1.cpu(), data2.cpu(), 'Before update')\n",
    "\n",
    "    optimizer = torch.optim.SGD([model.w, model.b], lr=step_size)\n",
    "    objective_history = []\n",
    "\n",
    "    for i in range(num_update):\n",
    "        cost = model.get_loss(data, labels)\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        objective_history.append(float(cost))\n",
    "#     print(objective_history)\n",
    "\n",
    "    draw_state(model.w[0].cpu().detach().numpy(), model.b[0].cpu().detach().numpy(), data1.cpu(), data2.cpu(), 'Updated boundary')\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(objective_history)\n",
    "    ax.set_title(\"objective function\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data with '1' is appended: [X, 1]\n",
    "data = torch.cat([data1, data2], axis=0)\n",
    "labels = torch.cat([torch.ones(datanum1, device=dev), torch.zeros(datanum2, device=dev)])  # label of class 1: 1, label of class 2: 0\n",
    "\n",
    "model = LogisticRegression_torch1(input_dim=dim, output_dim=1).to(dev)\n",
    "update_w_torch1(model, data, labels, num_update=100, step_size=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_w_torch1(model, data, labels, num_update=10000, step_size=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use torch.nn.Module parameters\n",
    "\n",
    "Now the variables are modified from Torch tensor to those generated from torch.nn.Parameter. The remaining parts remain unchanged.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For test 2\n",
    "class LogisticRegression_torch2(torch.nn.Module):    \n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression_torch2, self).__init__()\n",
    "        self.w = nn.Parameter(distributions.Normal(0, 1).sample(sample_shape=[1,dim]))\n",
    "        self.b = nn.Parameter(distributions.Normal(0, 1).sample(sample_shape=[1,1]))\n",
    "    def get_loss(self, data, labels):\n",
    "        fs = self.forward(data)\n",
    "        loss_val = torch.sum(-labels*torch.log(fs) - (1 - labels)*torch.log(1 - fs))\n",
    "        return loss_val        \n",
    "    def forward(self, x):\n",
    "        outputs = torch.sigmoid(torch.matmul(self.w, x.T) - self.b)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the update, optimizer is changed to keep the model.parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_w_torch2(model, data, labels, num_update=100, step_size=0.001):\n",
    "    draw_state(model.w[0].cpu().detach().numpy(), model.b[0].cpu().detach().numpy(), data1.cpu(), data2.cpu(), 'Before update')\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=step_size)\n",
    "    objective_history = []\n",
    "\n",
    "    for i in range(num_update):\n",
    "        cost = model.get_loss(data, labels)\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        objective_history.append(float(cost))\n",
    "#     print(objective_history)\n",
    "\n",
    "    draw_state(model.w[0].cpu().detach().numpy(), model.b[0].cpu().detach().numpy(), data1.cpu(), data2.cpu(), 'Updated boundary')\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(objective_history)\n",
    "    ax.set_title(\"objective function\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data with '1' is appended: [X, 1]\n",
    "data = torch.cat([data1, data2], axis=0)\n",
    "labels = torch.cat([torch.ones(datanum1, device=dev), torch.zeros(datanum2, device=dev)])  # label of class 1: 1, label of class 2: 0\n",
    "\n",
    "model = LogisticRegression_torch2(input_dim=dim, output_dim=1).to(dev)\n",
    "update_w_torch2(model, data, labels, num_update=100, step_size=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After $10000$ more iterations,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_w_torch2(model, data, labels, num_update=10000, step_size=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use torch nn.Linear module\n",
    "\n",
    "Now the coding with pyTorch can be simple if we find an appropriate pre-implementation in the pyTorch library. The implementation of our linear operation, $\\mathbf{w}^\\top\\mathbf{x} - b$ can be replace using \"torch.nn.Linear\" module. We simply obtain the \"torch.nn.Linear\" module and use the parameters inside the Linear module for the update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For test nn.Linear\n",
    "class LogisticRegression_Linear(torch.nn.Module):    \n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression_Linear, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "    def get_loss(self, data, labels):\n",
    "        fs = self.forward(data)\n",
    "        loss_val = torch.sum(-labels*torch.log(fs) - (1 - labels)*torch.log(1 - fs))\n",
    "        return loss_val        \n",
    "    def forward(self, x):\n",
    "        outputs = torch.sigmoid(self.linear(x)).T\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, direct access to the variables in the module is naturally limited. The programming is becoming more like a black box. In order to investigate the learning process, you can replace values of the parameters and access the values of the parameters using the following methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a class having the nn.Linear module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression_Linear(input_dim=dim, output_dim=1).to(dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put your own value in the nn.Linear parameter and see if you can calculate the loss function using the updated values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=====Set a nn.Linear parameter and find the calculated loss=====')\n",
    "model.linear.weight.data[0,0] = 1\n",
    "model.linear.weight.data[0,1] = 1\n",
    "# model.linear.bias.data[0] = 0\n",
    "model.linear.bias.data[0] = -1\n",
    "print('weight: \\n', model.linear.weight.data, model.linear.bias.data)\n",
    "print('Model loss: \\n', model.get_loss(data, labels))\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "cost = model.get_loss(data, labels)\n",
    "optimizer.zero_grad()\n",
    "cost.backward()\n",
    "optimizer.step()\n",
    "print('Updated weight: \\n', model.linear.weight.data, model.linear.bias.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above calculation should be confirmed using our own calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n=====Without Torch=====')\n",
    "extw = torch.tensor([[1., 1., -1.]], device=dev) # b = -extw[2]\n",
    "print('weight: \\n', extw)\n",
    "extX = torch.cat((data, torch.ones([len(data), 1], device=dev)), axis=1)\n",
    "ts = torch.matmul(extX, extw.T).T  # w^TX\n",
    "print('Conventional loss \\n', get_loss(labels, logistic_function(ts)))\n",
    "fs = logistic_function(ts)\n",
    "extw = extw + 0.01*(torch.matmul(labels - fs, extX))\n",
    "print('Updated weight: \\n', extw)\n",
    "\n",
    "print('\\n========== Compare updated bias ============')\n",
    "print(model.linear.bias.data, 'vs', extw[0,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the update function, we use model.linear.weight instead of model.w or model.b, as it is the official name of the nn.Linear module. Without parameter investigation (for only learning), we can use the previous update function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_w_torch_Linear(model, data, labels, num_update=100, step_size=0.001):\n",
    "    draw_state(model.linear.weight[0].cpu().detach().numpy(), \\\n",
    "               -model.linear.bias.cpu().detach().numpy(), data1.cpu(), data2.cpu(), 'Before update')\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=step_size)\n",
    "    objective_history = []\n",
    "\n",
    "    for i in range(num_update):\n",
    "        cost = model.get_loss(data, labels)\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        objective_history.append(float(cost))\n",
    "#     print(objective_history)\n",
    "\n",
    "    draw_state(model.linear.weight[0].cpu().detach().numpy(), \\\n",
    "               -model.linear.bias.cpu().detach().numpy(), data1.cpu(), data2.cpu(), 'Updated boundary')\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(objective_history)\n",
    "    ax.set_title(\"objective function\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data with '1' is appended: [X, 1]\n",
    "data = torch.cat([data1, data2], axis=0)\n",
    "labels = torch.cat([torch.ones(datanum1, device=dev), torch.zeros(datanum2, device=dev)])  # label of class 1: 1, label of class 2: 0\n",
    "\n",
    "model = LogisticRegression_Linear(input_dim=dim, output_dim=1).to(dev)\n",
    "update_w_torch_Linear(model, data, labels, num_update=100, step_size=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_w_torch_Linear(model, data, labels, num_update=10000, step_size=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    "\n",
    "We now put all together and make a class to perform Logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(torch.nn.Module):    \n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "    def get_loss(self, data, labels):\n",
    "        fs = self.forward(data)\n",
    "        loss_val = torch.sum(-labels*torch.log(fs) - (1 - labels)*torch.log(1 - fs))\n",
    "        return loss_val        \n",
    "    def two_norm(self):\n",
    "        outputs = torch.sum(self.linear.weight**2)\n",
    "#         outputs = torch.sum(self.linear.weight**2) + torch.sum(self.linear.bias**2)\n",
    "        return outputs\n",
    "    def get_loss_2norm(self, data, labels, reg_const = 1.):\n",
    "        fs = self.forward(data)\n",
    "        loss_val = torch.sum(-labels*torch.log(fs) - \\\n",
    "            (1 - labels)*torch.log(1 - fs)) \\\n",
    "            + reg_const*self.two_norm()\n",
    "        return loss_val        \n",
    "    def getExtw(self):\n",
    "        return torch.cat([model.linear.weight[0], model.linear.bias])\n",
    "    def linear_out(self, x):\n",
    "        outputs = self.linear(x)\n",
    "        return outputs[0]    # make predictions\n",
    "    def forward(self, x):\n",
    "        outputs = torch.sigmoid(self.linear(x)).T\n",
    "        return outputs\n",
    "\n",
    "model = LogisticRegression(input_dim=dim, output_dim=1).to(dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_w(model, data, labels, num_update=100, step_size=0.001, reg_const=1):\n",
    "    draw_state(model.linear.weight[0].cpu().detach().numpy(), \\\n",
    "               -model.linear.bias.cpu().detach().numpy(), data1.cpu(), data2.cpu(), 'Before update')\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=step_size)\n",
    "    objective_history = []\n",
    "\n",
    "    for i in range(num_update):\n",
    "        cost = model.get_loss_2norm(data, labels, reg_const)\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        objective_history.append(float(cost))\n",
    "#     print(objective_history)\n",
    "\n",
    "    draw_state(model.linear.weight[0].cpu().detach().numpy(), \\\n",
    "               -model.linear.bias.cpu().detach().numpy(), data1.cpu(), data2.cpu(), 'Updated boundary')\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(objective_history)\n",
    "    ax.set_title(\"objective function\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data with '1' is appended: [X, 1]\n",
    "data = torch.cat([data1, data2], axis=0)\n",
    "labels = torch.cat([torch.ones(datanum1, device=dev), torch.zeros(datanum2, device=dev)])  # label of class 1: 1, label of class 2: 0\n",
    "\n",
    "model = LogisticRegression(input_dim=dim, output_dim=1).to(dev)\n",
    "update_w(model, data, labels, num_update=100, step_size=0.001, reg_const=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_w(model, data, labels, num_update=10000, step_size=0.001, reg_const=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that with regularization, the loss function arrives at a plateu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various methods to debug the modules can be used. For example, we can investigate the output of the Logistic regression function using 'forward' function. We can make our own function for debug, such ans 'linear_out' in this class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.linear.weight.data[0,0] = 1\n",
    "model.linear.weight.data[0,1] = 1\n",
    "# model.linear.bias.data[0] = 0\n",
    "model.linear.bias.data[0] = 1\n",
    "intput_sampe = torch.ones([1,2], device=dev)\n",
    "print('input sample: \\n', intput_sampe)\n",
    "print('weight: \\n', model.linear.weight.data, model.linear.bias.data)\n",
    "print('Network output:\\n', model.forward(intput_sampe))\n",
    "print('Linear output:\\n', model.linear_out(intput_sampe))\n",
    "\n",
    "data_temp = torch.tensor([[1,0],[0,0],[1,1],[0,0]], dtype=torch.float32, device=dev)\n",
    "labels_temp = torch.tensor([[1,0,1,0]], device=dev)\n",
    "print('Model Loss: \\n', model.get_loss(data_temp, labels_temp))\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For test\n",
    "ts = torch.matmul(model.linear.weight, data_temp.T) + model.linear.bias\n",
    "print(torch.sigmoid(ts))\n",
    "print(logistic_function(ts))\n",
    "print(model.forward(data_temp).T)\n",
    "print('Model loss', model.get_loss(data_temp, labels_temp))\n",
    "print('True loss', get_loss(labels_temp, logistic_function(ts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can access the variables using the following variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(input_dim=dim, output_dim=1).to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.linear.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.linear.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can test the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_loss(data=torch.tensor([[1,0],[0,0],[1,1],[0,0]], dtype=torch.float32, device=dev), \\\n",
    "               labels=torch.tensor([[1,0,1,0]], device=dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With different labels,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_loss(torch.tensor([[1,0],[0,0],[1,1],[0,0]], dtype=torch.float32, device=dev), \\\n",
    "               torch.tensor([[1,1,1,0]], device=dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
